So mainly from a beginner perspective, this is your whole system design. This is how you design a scalable design system, in which all the basic components are covered. Alright, so, hey everyone, welcome back, welcome to another exciting video. And in this video, we are going to study system design. So in this particular video, we will understand the components of system design And in reality, how things work in production Load balancer, API gateway, caching, queue system, broker systems And your high throughput systems, how do they work? What is microservice architecture? What are container orchestrations? And all these things, how do they work in cloud environment? What are application load balancers? What are network load balancers? Where should someone use it? What is VPC? So, we are going to understand all these things in this particular video. And we will see that how many components are there in system design, how to do rate limiting, how to scale out, how to scale in and we will see what is horizontal scaling, what is vertical scaling, how scaling is managed, what are auto scaling policies, so we will cover all those things in this particular video. So, it is a very beginner friendly video in which we will see how we work all the components system design ke andar, kis tarah se mil kar kaam karte hain and ek robust system banatein, that is what we are going to understand in this video. So with that, let's start with the video. Alright, so this video ko shuru karte hain ek story se, ek real world story se, for example, hum Amazon ki story le sakte hain, ki agar aap ek Amazon jaisa, ek e-commerce application banare ho, jinka traffic is massive and global level ke upar operate karte hain, to ye cheeze kaise kaam karte hain. To dekho, system design ke andar, ek bohot hi high level ke upar humare paas sirf do cheeze hoti hai. Number one is your client. Client kaun hota hai? Jo bhi aapke application ko use kar raha hai. To ye ek mobile device ho sakta hai, ye ek laptop ho sakta hai, ye ek IoT device ho sakta hai. Basically client jo bhi aapke server ko use kar raha hai. And on the other hand you have a server. Okay. So let's say ye aapka kya hai? Ye aapka ek server hai. Server kya hota hai? Server is nothing. Ye bas ek machine hai. Ye bas ek machine hai. Jo capable hai 24 by 7 run karne ke. Right? 24 by 7, it can run this. And it has a public IP address. Let's say, the public IP of this machine is something like this. So, this public IP is publicly accessible. Means, any one in the world can type this IP address and reach my server. So, this is a server. Is it necessary for the server to be on cloud? No. Because if we understand cloud, cloud is nothing. It is just someone's machine. So technically this server can be at your home Your laptop can also act as a server Your personal computer can also act as a server If you can keep it 24 by 7 up Because it needs a power connection Plus you can assign it a public IP So of course you don't want to keep your laptop 24 by 7 on And you don't want your personal machine to be connected to a public IP So that's why companies like AWS, DigitalOcean, etc. come It basically gives you a virtual machine which is running 24 by 7 and it gives you a public IP. So, what is this? This is your server and your client can request on the server using this IP address. It cannot reach your server without IP address. Right. Now, you will be like, what is this IP address? IP address is basically an address. Like, there is an address of your house. You have a street name. You have a street number. There is a number of your house. After that, there is a city. is a pin code of that city. Then you have a state. It has a state code. Then after that you have a country. Right. So similarly, if you want to reach this server on the internet because internet is massive, then you need an IP address. But the problem here comes is that this IP address is very hard to remember. It is very difficult to remember. Do you ever remember your phone number of your friends? No. So can we give it a user friendly name? Can we give it a human friendly name which is easy to remember? For example, Amazon.com is relatively easy to remember rather than remembering 10.2.3.4. So, to solve this particular problem, we have the next component in system design, that is a DNS server. Okay, DNS server. So, let's say this is our DNS server. So, what does DNS server do? DNS server is nothing. It is a global directory. What is the directory of which domain? Which IP address? So this directory remembers, it's just like key value pair, like your telephone directory, that this is the IP of Amazon.com. Similarly, what is the IP of Google.com? So when you type Amazon.com in your browser, your first request actually goes to DNS. And you ask for the IP address of Amazon.com. DNS returns you the IP address. And now because you have the IP address, you can send your request to the correct server. you can send your request to the DNS server and also you can receive a response. So, this particular process is called DNS Resolution. So, this is called DNS Resolution. So, whenever you buy any domain and you enter IP address on it, basically you have to pay a fee which goes to the DNS server. So, DNS server works in a decentralized way. And the full purpose of DNS server is that it is a directory, a global public directory that which domain has what IP. So, this is the DNS server. Now, let's say you have kept this server up and running This is up and running Now, in reality, this particular machine At the end of the day, it's a physical machine It has a CPU and RAM Let's say you have two CPUs and 4GB RAM And this is powering your system Now, what happens is Over time, your users will increase So, it means that the load of the request on you will increase. And there is a high probability that 2 CPU 4GB RAM will become a bottleneck. Your server will be overwhelmed due to which the server can crash. If it is out of memory, if you have used 4GB RAM, then this server can crash. That's why we say that when the load increases on the website, the website crashes. Server down hai. Toh wo server down ki ho hota hai? Because uske upar jo rush haya, uska jo physical resources the, I'm repeating, uske jo physical resources the, wo utne enough nahi the ki wo itna load handle kar pae. Tabhi aapne dekha hoga, jab exam ka result aata hai, sabhi students, NCRT ya phe, jo bhi aapka board hai, jab aap uske upar jahate ho, right? Request karte ho bohi zada, toh unka server down ho jata hai. Toh, of course, server down hona is not a good thing. Tikhe? So what we should do is we have to make it fault tolerant in some way. So what can we do to make it fault tolerant? Let us say if I know that I can get a lot of rush, then I can increase the resources of this machine. What I will do is I know that my website is very active, so it is possible that I will put 8 CPUs on it, not even 8, let's say we take a decent number here, 64 CPUs, which is a very big number. And what I did here is I put 128 GB RAM. Now you can see that this machine has become a powerful physical machine. 64 CPUs and 128 GB RAM. So that means it can handle a lot of traffic. It can handle a lot of traffic but still, there will be some limit. If all the global users come, then this server will crash at some point. So this is also a problem. So in that case, you can upgrade it again. Let us say, you anticipated that if all the users in the world come to your website, then also down off. So you took a very dumb decision. Let's say money was not a problem. You installed 6,000 CPUs and you installed 128,000 GB RAM. Now this is a super, ultra super computer. You made it. Now as many users as you want, there is no problem. But in reality, does it really always remain this much load? If you look at the traffic pattern of Amazon, does it always have a load on it? No, it increases after coming to festival seasons. Okay, when there is a recession, the load becomes less. So technically, what you are doing is you are wasting resources. You are not using it efficiently, right? Because what you did is you upgraded it for the best case. So in this particular scene, what are you doing is you are kind of wasting your money. You're wasting your resources because 90% of the times you're using it only 10%. Let's say. Okay. So, what you did was you over-optimized it. By the way, this is called vertical scaling. What do you call it? Vertical scaling. What you're doing in vertical scaling is you're basically increasing the resources of your physical server. You're increasing the CPU. You're increasing the RAM. Which we call vertically scaling. So, vertical scale means adding more resources to your server. So, let me show you the definition. Vertical scaling. So, the meaning of vertical scaling is that vertical scaling also known as scaling up involves increasing the capacity or capabilities of a single hardware or software component. This is done by adding more resources like CPU, RAM, disk space to an existing server or a virtual machine. So, this is called vertical scaling. Okay, nice. Now technically, the clouds like AWS What do they say? They say that you don't need so many resources every time What you can do is You tell me that ideally you only need 2 CPUs And I only need 4GB RAM But if my traffic increases If my traffic increases Then please keep increasing its CPU and RAM So what will happen in that? You won't have to pay more every time The more time you spent on your website, the more resources you consumed, the more hours you paid for it. And when your rush was very less, you paid less for it. Let me give you an example. For example, if you are running a restaurant system. So, you see, there is no rush in a restaurant. Correct? So, let's say you have two cooks. So, how much salary do you have to pay? You have to pay salary to two cooks. But if there is a rush, then you think, I will hire 10 cooks. But if you hire 10 cooks, You will have to pay 10 cooks per month. But rush is not always at peak. So, what you can do is, you can take it from someone at rent. You are hiring a cook on a per hour basis. That, look, rush is not there. But, okay. I have rush now. Please come. I will pay you extra for the hours of work. And after that, you go. So, it's basically that kind of stuff. That, minimum, I will employ this much. That is 2 CPU, 4GB RAM. But, if my rush increases, I will increase my resources. Now, vertical scaling is good. but here is a downside. You cannot do vertical scaling in a running machine. Can you add CPUs and RAM in a running machine? No. Machine has to restart if you are adding physical resources. Machine has to physically restart. That means there will be a downtime. So what is the problem of vertical scaling? There is a downtime in it. So if there is a spike suddenly, here understand carefully, if there is a spike suddenly, suddenly people came, that 5 minutes ago there were no people, But after 5 minutes people came, so there was a spike. So now what will happen in this case? First your server will be closed, new resources will be added in it, then your server will restart. But tell me one thing, if there is a sale going on on Amazon, let's say big billion days, and it will open at 12 o so if you think there will be no rush till 11 o because the sale will not start at will start As soon as 12 12 12 will happen there will be a peak rush and at that time think their server will be closed because it is scaling up And after that, it will take some time to restart. Even if it takes one minute. Can Amazon bear one minute downtime? No. You need to have a zero downtime. So, what is the problem of vertical scaling? problem is that here comes a downtime because you cannot add resources in the running machine. So, what is its solution? Its solution is that we will not do vertical scaling. What we can do is let's repeat this example again. This was our server and initially these are our resources. What I can do is, if this machine has an IP, of course, right? This machine has an IP. Let's take this IP. So, if the load is increasing on it, can I parallelly spin up a machine? do? Parallelly, do another machine spin up? Look, this machine will also take time to up, right? This machine will also take one minute to up, but till then at least this machine is active. Now, at this time, my two machines are active, but as soon as the rush increases, what will I do? I will spin up another machine. So, till the time this machine is booting up, till then these two machines are handling my server. So, what will happen? In between, my website will slow down a little, but at least I have zero downtime. Now, here is a problem. If you scale up scale. If you scale like this, which is called horizontal scaling. Okay, this is called horizontal scaling. Okay, in horizontal scaling, what you do is you add more servers. You don't increase the physical resources of one server, you add more servers and replicas of that server. Now the problem with this is, see, the IP address of this machine will be something, let's say 5, the IP address of this machine is 6. Now tell me one thing, when the user came, okay, it typed amazon.com and DNS returned this IP because DNS can return only one IP. So, all the users are still pointing here. All your global users are still pointing at this IP. How will you map your domain to amazon.com that someone gets this IP, someone gets this IP, someone gets this IP. How will you distribute the traffic is a problem. Here I have three different machines, three different IPs. How do I distribute it? So, whenever you scale horizontally, that means you increase replicas, you have to put another server in between, which is your load balancer. Okay. This is a load balancer. Load balancer will also have its own IP. Let's say, this is the IP of my load balancer, which is 10.2.3.7. So, what we do is, we register the load balancer IP inside our DNS. Okay. So, what will happen now, And whenever someone globally opens Amazon.com, then the request of that will be routed to which server? It will be routed to our load balancer. Now, what is the work of load balancer? The work of load balancer is to balance the load coming in these servers. That is, which request will come to which server? Now, whose work is that? That is the work of load balancer. So, this is your load balancer. That is why it is called load balancer. Now, load balancers have different algorithms. because the problem is how to distribute it so load balancer has different algorithms the simplest algorithm is round robin round robin means first request on first server second request on second server third request on third server fourth request on first, second, third first, second, third, first, second, third so what it does is it equally distributes load among the servers if your traffic increases more you can do one more server spin up then tell load balancer tell load balancer that I have another server. Now load balancer knows that there are 4 servers and it will load balance in 4. And when your traffic will decrease let's say sales are stopped, you can basically close them. Now load balancer knows that you have 2. So whenever you load balancer and you install this, then you have to register it inside load balancer. So whenever a new server comes, first it boots. Correct? He slowly boots. And after that, you tell the load balancer. Then load balancer checks whether this server is healthy or not. Healthy means, is this server up and running for at least, let's say, one minute. If it is healthy, if it is ready to take the traffic, then it starts sending traffic to it. So, that is the intelligence of load balancer. In Amazon world, it is also called ELB. That is Elastic Load Balancer. So, this is the terminology of Amazon. But at the end of the day, what is this? This is an elastic load balancer So this is your load balancer Now you will be like, load balancer can crash here too Absolutely it can But when you use elastic load balancer Which is Amazon managed It has a lot of micro machines A lot of threads, workers So they handle efficiently So basically this single server Has got good resources And load balancer can load balance your traffic So this is how your load balancer works So, you have to put a load balancer in front of horizontally scale. In vertical scaling, you don't need a load balancer because you are increasing the resources of one server. So, you still have one IP. Now, next thing. See, technically, you have a lot of services in microservice architecture. Let us say, I have an authentication service. Let us say, I have a different API service. Let's say, I have a different service for orders. I have a different service for payments. Now what happens is, look, what is AuthService doing? It is working to authenticate the user. What is OrderService doing? It is doing order management. So now what will happen? All these will have their own servers. So let's say AuthService is something which has a lot of servers. So what is AuthService doing? It is horizontally scaling. So what did you do? You scaled out AuthService to 4 servers. Let's say orders are scaling themselves. So let's say your orders have 3 servers. Let's say in the case of payments, you have two servers running. Because people browse a lot, but how many of them are buying is less now. And API is being used the most. So what is API? You have a lot of servers. So let's say you are at a point of time running six servers. Now what will happen is, what do you want? If any user requests on slash auth, let's say amazon.com, Amazon this is your root domain .com slash auth so you want in the case of slash auth this request should technically go to these servers this auth service should go to on the other hand let's fix the errors ok if a request is coming on this domain then you want that request to come here if a request is coming on slash orders slash something something like anything can happen here so you want that request to come here if it is payments slash something like payment slash get payment by ID something like that, so you want it here and if it is a request of Amazon.com then it should come here, so you basically want to do routing here that you come here on slash auth slash orders, slash payments so how can you do that for that what you have to do first let's understand a little, see my first step is I want here a server which on host basis or on my route basis pe routing kar paaye. Route, routing. Okay. Route, routing ka matlab kya hota hai? Ki if it is a slash auth, to yaar ek kaam karo auth service pe jao. Okay. Auth service pe jao. Okay ji. Yeh hamara rule ban gaya. Okay. Uski baad fir mene bola yaar if it is slash orders. Okay. To ek kaam karo order service pe jao and so on. To yaha par auth service pe to jao. Let's say apne router bana liya. Okay. Lekin iske kaunse server pe jao. Fir wohi problem aagayi na. Which server should I go to? So technically, what we can do here is We will put our own load balancer of auth service Okay, so this is load balancer So here is a load balancer Which is doing load balance In between this One second, let's tick the arrows a little In between this In between this Okay, so here this load balancer is doing Let me just correct the arrows Right And what you can do is You can tell this routing If request comes on slash auth request should be going to this particular load balancer. Similarly, one more load balancer will be used for orders. So, this is here, this is here and this is here. This load balancer is doing it in between its own services. Let me just set it. And you can say slash orders request will go here. So, this is how you route microservice and this thing is called API Gateway. What is API Gateway? So, let's take the API Gateway logo. This is your API Gateway. So, this is the correct logo for API Gateway. So, what is API Gateway doing? API Gateway is basically looking at which service we should route it over. So, if it is slash auth, then it will point it on the load balancer of the auth service. Now, what will the load balancer do? It will assign the request to any server. And these servers usually are EC2 machines. So, let's take the logo of EC2 here. EC2 machines. EC2 is Elastic Compute. In Amazon's terminology this is your VM machines. So here we use these logos because this looks more cleaner. So that means API gateway has its own public IP let's say its IP is 10.5.8.3 so that means your DNS will have this particular IP registered. So any request that goes to amazon.com, you will route it to your API gateway. API gateway internally route to the correct load balancer. API gateway will forward to the correct load balancer based on some rule. You must have written a rule. And then load balancer will work to load balance all the replicas. So load balancer and API gateway work together. So let's search here API gateway. So if you look at API gateway and go to images, so you can see this is how the API gateway is working. So, what is happening in API gateway? Client came to API gateway. Now, API gateway, see what routing it did. It can bring service A and service B. Service B will have its own load balancer. It will have its own services. So, if we open this diagram here, then you can see Amazon API gateway. If request slash service 2 goes, then you have invoked a lambda function. If service 1 goes, then you have done a load balancer, behind which was an EC2 machine. If it goes to slash docs, then you pointed it at S3 and if it is slash servicely then it is possible that this request is going somewhere outside. So this is the work of API gateway. So if you want to read its definition. An API gateway is a centralized entry point for API calls acting as a reverse proxy that routes requests from clients to the backend services and back. So this is your API gateway. And here you can attach an authenticator with API gateway. So let's say you have an authenticator service. Let's say Auth0 you have an Authenticator service, you have any. So basically, you can authenticate the user and then you can route it to the correct thing. So here, all your rules come. Nice. Now, when you make such a big, robust system, of course, there will be some batch processing. What does batch processing mean? Let's say, you had a vendor in Amazon's case, he uploaded a bulk list. Or you have to trigger an email to 1 million users that sale is active. So technically can you run 1 million loops in a server No of course not right So for these things what do you have You may have some background services Let say you have made a service What is this This is your email service What is this? This is your email service. Okay, email service is not said. Let's say, let's call it email worker. What does this do? It reads all the things from the database and emails them. Because email worker, see, it will take time to send email to 1 million users. time will take. Or it could be someone has uploaded a bulk list, a bulk excel file and you have to insert it. So, this is a bulk worker. Now, what these workers do? These workers are basically running in the background. They run in the background. Now, let's say, if someone has done payment, then you want to trigger something, email. So, one way is, what this payment service will do internally, internally, this email worker will call the user that a user has made a payment, please send him an email. So, he will send the necessary details like order ID, his email ID, what to send, all that. And what will this worker do? This worker will be interacting with some external API. For example, let's say we take the example of Gmail. So, what did this email do? It talked with the Gmail API, sent the email to the user, whatever response came, it returned it. So, this is one way of doing it. Now, here is a problem. This method is called is synchronous way. What does it mean? Synchronous way. What you did is you called it synchronously. You may have called an HTTP post here to an email worker. Email worker sent it and came back. So, what happened synchronously? Your payment server had to wait till the email is sent. It has to wait. It is waiting for the response. So, tell me one thing. If every second someone is getting payment, every second, then is this a scalable solution? No. Because what will happen? you are waiting so what you can do is you can make it async because it can take time to send email email worker is dependent on gmail APIs it can take time so in this particular scenario usually what we do is we have to set a communication system between them which is asynchronous so most commonly what is used is queue system so what you can do is you can make a queue what is queue? first in first out so what can this do? payment services basically in this queue Let's say this is our queue, it will keep pushing in this queue whenever an order is placed. Whenever an order is placed. One order is placed and it has put the order id. One order is placed, so what is it doing? Every second, there are many orders coming. So it has put many orders like this. Correct? It has put many orders. And what can this email worker do? This email worker will basically pull an event from this queue, send an email and discard it. Then it will pull the next one, send an email and discard it. you might have noticed that when you place an order, the email usually comes after 2 or 3 seconds. Why? Because the email worker is running. And as the rush increases, you can also scale this email worker horizontally. So, this increases your parallelism. Parallelism means, first you were processing one email at a time. Now, what will happen? Now, you are able to process two emails at a time. Correct? If you add more, you will be able to do more. And, do you know what is another advantage of this architecture? see, you are dependent on an external service here. Whose service? Gmail. Let's say Gmail told you, I will only allow you to send, let's say, 10 emails per second. What is this? This is your rate limit. You can send 10 emails in 1 second, not more than that. So technically, if you use this system, you can create a bottleneck here. See, you have a lot of users. are coming. Your system is very robust. You have millions of orders placed. So what you do is you basically queue their email. You set this email worker that do one thing please in one second in one second never pick more than 10 emails. Never pick more than 10 emails. Never pick more than 10 emails. So what happened with this? Basically you created a bottleneck which will never will not complain to you that you are hitting my rate limiting. So usually Q system is used so that you can communicate two microservices together. This is completely async. So for this, in Amazon you have SQS, Simple Q System. SQS is a service which is a very robust service, you can do SQS. Now, when you introduce any Q system, a different problem arises, how will this pick up the event? So in this you have two mechanisms, push mechanism and pull mechanism. Pull mechanism is when an email worker picks an event from here. So, what can he do? He can poll. Polling means, every 2 seconds ask, Hey, do you have any message? Then this worker will be asking our Q, Hey, do you have any message? If you have, then give it. What happens in push mechanism is basically Q invokes your worker. So, that is a push mechanism. So, he is pulling and if it is pushed, then it will be like this. So, this is a push mechanism. In SQS case, you have to pull mechanism. You have to poll. Okay. So, you can do long polling or short polling. Short polling means request in every second. Is there anything? No. Is there anything? No. Is there anything? No. In this, your API calls increased. But what will be the problem? In this, your cost will also increase. In long polling, wait for 10 seconds. I will wait for 10 seconds. Or something is there, block it for 10 seconds. In 10 seconds, pull all the events at once. Process it. Then block it for 10 seconds. So, this is your long polling. Okay. So, these are our email workers. Similarly, you may have another queue. Let's say CSV queue. So, if a user has uploaded some bulk, then you put it in CSV queue and it is listening to it and it is doing its work. It is possible that you have a database of MongoDB. So, what will it do? It will process it one by one and will keep inserting it in MongoDB. So, here comes a different worker. If you want to increase parallelism in this, then do one thing, put another worker. There you go, done. So, queue system. Now, there is a problem. Okay. Many times what happens is that when there is an event, let's say you have made a payment. So usually you don't just do one thing. When a payment is made, you want to trigger it with WhatsApp message, you want to do it with SMS. You want to send an email to the vendor and you want to send an email to the user. So on one event, I am using the word event here. On one event, meaning on one payment, you want to do four things. So in such cases, what you can do, you can do something known as a PubSub model. You can publish it. You can publish a notification. So instead of a queue system, what you could do? You have a pub sub mechanism here. So simple, I am forgetting the name. Simple notification service, SNS. So what does SNS do? What can you do inside SNS? This is your simple notification service. What will payment service do? It will send a notification on it. Brother, someone has paid. Brother, someone has paid. Now, what can happen? You have different micro services. that email worker was also listening to it. Email worker was also listening to it. Okay. Let us say your WhatsApp API is running somewhere. Okay. WhatsApp's microsystem is running. So, he was also listening to it. You triggered some email and you triggered WhatsApp too. Let's say some SMS system is running. So, let's say some SMS system or some other microservice is running. He also listened to it. So, what happens in PubSub? You give it to one. Multiple people can receive it. Multiple services can receive it. But in SQS, what happens is one to one. So, if we put another worker here, if this event gets this, if this gets this, then it will never get this. Okay? In SQS, what happens is you push an event, only one consumer will pick it up. Meaning, always, it guarantees that that message will be processed only once. But, if you want to publish it, that I did one message, everyone should pick it. If you can see the diagram here, one message goes to many people, many services, then you can also do PubSub model. So, this is called Event Driven Architecture. That some event happened, user made payment, you emitted that event. User made payment. It's like you made announcement, user made payment. Now, whoever was listening to it, they can work in their own way. Okay? So, this is called Notification System, PubSub or Event Driven Architecture. What is the problem in Event Driven Architecture? that here there is no acknowledgement. Okay, what is acknowledgement? Look, if a user has made a payment, it is very important to send an email. Okay, it is very important to send an email. If you use QSystem, then look, you have picked this event. Now, can it be that when you were trying to send an email, at that time, Gmail was down. Mail was not able to be sent for some reasons. So, what can you do? You either enqueue it back. So that it can be processed later. Or you can add dead letter Q. DLQ, dead letter Q. That this message failed in processing. And maybe you can retry it after 5-10 minutes. So, in this, there is an acknowledgement. That yes, the message was acknowledged. Received. But when you do event-driven architecture, you send it. It is possible that on the other hand, no one was listening to that event. It is possible that when you put an event here that a user purchased something, he picked it but it failed. So that message got destroyed. So here there is no acknowledgement. So in queue system, you get a guarantee that it will be acknowledged or you can push it in dead letter queue. In notification event driven architecture, you published an event, it is possible that someone was listening to it, it is possible that someone was not listening to it, it is possible that the one who was listening could not process that message, so there is no retry mechanism. then you have to build it yourself. So this is basically what? This is your pub sub. Not pub sub I would say, this is basically your notification. Event emitting. Now there is something known as a fan out architecture which is used a lot in microservices. What happens in fan out architecture? Whenever there is an event, of course you don't want to do one thing, you want to do multiple things. If you want to do multiple things, then you will use SNS, Simple Notification Service. But there is a problem with SNS that there is no acknowledgement here. So to overcome this, what you do, you can do a fan out architecture. Let's say what we can do. Let's say this is my WhatsApp queue. Whatever message you will put in it, it will send the user to WhatsApp. Very good. Let's say you have another queue. This is your email queue. Whatever message you will put in it, it will send an email. Let's say you have another queue, which is your SMS queue. Now, these two have their own processors. So, let's take the logo of EC2. So, let's say you have this EC2 and this EC2. So, these both are basically listening for the WhatsApp queue. You have one more which is listening for the email queue. You have more servers which is listening for the SMS queue. Now, what you can do is, in the fanout architecture, you can take the notification. Take the notification service here. In notification service, you can actually bind these queues. You can actually bind these queues. That's it. Now, what will happen? Whenever a user is making a payment, and he will publish an event in it, that some user has made a payment. So, that event will automatically go to this queue, this queue, and this queue. Now, technically, it may go to WhatsApp message. It may go to email. SMS failed. It's okay, you can retry it. Because now this queue system here who is handling it It using SQS These are what These are your SQS So what happened with this With FanOut Architecture you were able to notify multiple people multiple services that something happened. But on the second hand, because you have pushed it in queue, you can track down that retry mechanism and all your acknowledgments here. So, this is your FanOut Architecture. So, you are searching FanOut Architecture. FanOut Architecture. So, you will get Amazon's own. So fan out architecture works like this. For example, you uploaded a video on cloud front. What did S3 do? S3, sorry, my bad. You will get one more. You uploaded something on S3. Now you want to tell multiple services. Let's say you are making a YouTube video. You uploaded a video on YouTube. Now when you upload a video, what can you do? You want to transcribe it, you want to process it, you want to make it 480p, 4k, you want to make it a thumbnail. Put it on SNS. Now, what did SNS do? Multiple. Multiple queues were invoked. That, man, transcribe it in 480. Transcode it in 360. Transcode it in 480. Transcode it in audio only. Make its thumbnail. Do this. And then, what you can do? You can put processors in front of them. So, this is your fanned out architecture. Because, a message came. And you have fanned it out in multiple services. Pub, sub, fanned out, patterned, and serverless. Okay. So, you can read such architectures. This is basically your fanned out architecture. Look. API gateway. API gateway invoked Lambda. Lambda published a message on SNS. SNS put it on multiple queues. And in front of these queues, there were different poll type consumers. So this is your fanned out architecture. So if you read these things, you will understand how a robust system is made. Very good. Let's move on. Now tell me one thing. When you interact with load balancers, then of course they can overwhelm your system. Do you agree? What does overwhelm mean? what happens? They can send fake requests. They can do DDoS, DDoS attacks. So, of course, you need to implement something known as a rate limiting. Okay? Rate limiting. Rate limiting means, how many requests can you send in a given point of time is known as rate limiting. So, you can implement a rate limiting here as well. For example, you can say that I will only process, or I will only accept 5 requests per second. If you try to do more than this, then I will start blocking you. I will start. Have you ever seen an error? Too many requests. So basically that will come. So rate limiting. Now rate limiting is not just that you are rate limiting the user. Rate limiting also has other meanings. You can put a rate limit in load balancer that let all the requests come. But send my servers in a controlled way. So here you have multiple rate limiting strategies. For example, you have leaky bucket rate limiting. You have token bucket. Token bucket rate limiting. So these are your rate limiting strategies so that your system is not exploited. So let's do one thing, let's search on Google rate limiting strategies. So here I'll show you an article, that's very good. API rate limiting strategies, this is actually my article by Piyushkar. So you can see that here you have rate limiting strategies. How token bucket algorithm works, so you can see that here with code, of course with code. And how can leaky bucket algorithm work, again with diagram and with code. Okay. So, you should read it. Which curve should be used? Read all that. And here, I have given real world examples that customer helpline happened. Right. Or YouTube video processing system happened. So, how to read it? So, you should definitely read this article. Okay. So, rate limiting is a very important concept. Otherwise, what will happen? On Amazon's scale, of course, many people will try to exploit it. Now, see what happens. When you are getting many requests, you do many computations. Means, you have a database. Let's say, this is your database. So, usually you have a database and all the micro services are querying or reading the data from this database. Now here is a problem. See, this single database will be overwhelmed. Because if your services are increasing, then what will happen? This database will be overwhelmed. So what you can do here is, there are multiple things to scale out a database. And the simplest way is that you can make its replicas. Read replica, write replica. So what we can usually do is, see, in the database there is a write operation, but you must have seen that in modern applications you have analytics analytics, how much you spent on it, so you can make multiple replicas of it multiple replicas of database which are called read replica read replica problem of read replica is that data is delayed a little bit and this is your master node. This is also called your primary node. So, if you want to insert anything, you will always have to insert here. You will always have to insert here. If you want to fetch data in real time, you also have to read from this primary node. But if there is an analytic service, there are logs in which there will be a little delay, then you can use read replica for that. This decreases the load on the primary node. So this is one way of optimizing the database. And the biggest thing is, if you do any complex computation, then there's always a good chance that you will cache it. So usually in caching, you have Redis, which are in-memory databases. So if you don't have anything available, first you check it in this. If available, you return it from here. If not available, then you go and ask the database. Cache the result and then return it from here. So, the benefit of caching layer is that you actually optimize and reduce the number of calls on the database. So, this is your caching thing. So, you can also use this. So, this makes your system more robust. So, if you see, we have covered many components to make a scalable system design. So, we have covered many things here. But there are still a lot more. There are still a lot more. In this, the whole system does not work. Okay. Let's go ahead and optimize it more. Can you see that I can see that this load balancer can be a problematic thing. So what we do is we remove this load balancer. We don't expose the load balancer directly. We can put a CDN in front of it. CDN stands for Content Delivery Network. See, technically what happens is that your users are in different locations. Correct? Some are from America, some are from Canada, some are from South India, some are from North India, some are from the left part, some are from the western part. So what happens in a content delivery network is that you have small micro machines all over the world. Of course, they can't be yours, they are made by Amazon. So what you can do is, you can create a content delivery network, CDN, also in Amazon's terms it is called Cloud Front. So Cloud Front. So what you can do is, you can deploy a Cloud Front. So this CloudFront distribution takes a little time to be made because it goes to every region and deploy a machine. So let me just do a CloudFront here. So let's say you have deployed many CloudFront machines all over the world. There are many machines. Now what happens is these all machines are basically internally routed to this load balancer. These all machines are internally routed to this load balancer. Now what you can do is Let's say This is your America's US's load balancer Sorry cloud front This is your North India Okay North India's Cloud front Okay Let's say This is your Maybe you know Canada side CA side So when I will make a request Look There are a lot of people in North India I will make users here There are a lot of people in North India Okay Of course Similarly, there will be a lot of people in Canada as well Just a second, copy paste In Canada as well, in US as well So, what we do here is When you request someone You basically request on your cloud front Your nearest cloud front Now, you will be like, Piyush, there are a lot of cloud fronts here Their IP will be different as well Their IP can't be the same So, how is it routed? Here, your anycast comes Anycast. Anycast what it does is, you can assign a single IP to all machines. And user is always routed to the nearest available Anycast. So, people of North India will be routed here. Let's say, I want to download a photo. I want to download a photo from the server. So, what it does is, on this machine, this edge here, it will check whether that photo exists in the cache or not. This user has ordered a photo, does it exist here or not? Let's say for the first time it doesn't exist. So now it will make a call to load balancer to the server. The information from the server, the photo will come here. So for the next time, CloudFront will cache it and give it to the user. So that means on the north India side, if anyone wants to read that photo again, if again, then he doesn't need to go to the server because in his region, someone has loaded that photo, then that photo will also be available with cache. What happened because of this? Number one, you could resolve the request very quickly. Because, it is possible that your load balancer was deployed in America. So, first you saved latency. That India's user request goes to America, you saved latency. Plus, because it was available in the cache, it got it from its edge computer. It got it very fast. It got it very fast. And, you didn't have to go to your server for its request. when it didn't go. Understood? Because it was in the cache. So, this request didn't go to your server. So, what you did is, you cancelled all the requests of North India. So, that way, the load on your server got even less. So, whenever you are watching a product on Amazon, there are some photos and videos of that product. Right? There are some photos and videos of that product. So, if that product has already opened in your region, its photos and videos have been downloaded from the server, and CloudFront has cached it, edge computers, they have cached it. Correct? So, all the other users will get it with the same cache, which will make their experience better, plus my network bandwidth will be saved. So, this is called CDN, Content Delivery Network. So, this is basically a CloudFront, with which you can make a more robust system, fast system and more optimized system. So, mainly from a beginner perspective, this is your entire system design. This is how you design a scalable design system in which all the basic components are covered. Now, I'm planning to make another video in which we'll cover advanced things. For example, containers, Docker, Docker containerization, container orchestrations, when multi-containers are orchestrated, how load balancers work, we'll cover all that. But on a basic level, if you know all these things, So I hope in this particular video, you must have learned something new about how a real-world system design works, what are the different components and how they work together and how they make a very scalable and highly available systems. okay so this was all about but this particular video a bigger friendly guide thought to the system design video castle a gum which a comment was a ruba Tana and a garage which or a CV does expect Karna Chateau yeah for you have some idea so do drop the comments so video castle a gum which a comment was a ruba Tana video got a salata like and subscribe to the canna milta a much video can there until then bye bye and take care .